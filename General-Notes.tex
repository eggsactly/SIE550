\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{amsmath}
\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\begin{document}

\section*{Definitions}

\begin{itemize}

\item Basis: The set of linearly independent vectors capable of representing all vectors in the space by linear combination (SIE 550 Week 1 Lecture Notes page 3).

\item Basis, Canonical: A basis who's vectors are perpendicular to each other. 

\item Controllable: A dynamic system with initial condition $x(t_0)=x_0$ is said to be controllable to state $x_1$ at $t_1(>t_0)$ if there exists an input $u(t)$ such that $x(t_1)=x_1$ ({\em Linear Systems Theory}, Page 243)  .

\item Decrescent: $v(\bar{x},t)$ is decrescent if $v(\bar{0},t)=0$ and if $\exists$ \\ a time-invariant positive definite function $v_e(\bar{x})$ such that $\forall t\geq0$, $v(\bar{x},t)\leq v_e(\bar{x})$.

\item Defective Matrix: A square matrix that is not diagnonizable and does not have a complete basis of eigenvectors, ie all the eigenvectors are not linearlly independent. 

\item Dimension: The number of vectors in the basis (SIE 550 Week 1 Lecture Notes page 3), ie the number of linearly independent vectors that represent all the other vectors in a vector space.

\item Hurowitz: When all eigenvalues of A are such that $\mathcal{R}_e(\lambda_i)<0$ A is called Hurowitz. 

\item Inner Sum (Direct Sum): Represented with the symbol $\oplus$ is a coordinate-wise sum. For example consider the sets $a=(x_1,y_1)$ and $b=(x_2, y_2)$, $a\oplus b=(x_1+x_2,y_1+y_2)$.

\item Isomorphic: Two vector spaces are {\em isomorphic} if they have the same {\em dimension}. This means isomorphism has two implications. $L:E\rightarrow F$ ($\bar{x}\rightarrow y=L\bar{x}$) such that:
\begin{enumerate}
	\item one-to-one $L(x_1)\neq L(x_2)$ if $x_1\neq x_2$ $\forall x, x_2 \in E_m$
	\item onto: For any $\bar{y} \in F, \exists \bar{x} \in E$ such that $L(\bar{x})=\bar{y}$
\end {enumerate}

\item Kernel: See { \em Null Space}.

\item Invariant: A set $G$ is an {\em invariant set} for a dynamic system if every system trajectory which starts in $G$ stays in $G$ for all future times. (Stability concepts (II) pages 1)

\item Limit Cycle: A limit cycle in the phase-plane of a second-order system is an isolated closed trajectory. 

\item Non-Singular: A matrix is non-singular if an inverse exists for the matrix. A square matrix is non-singular {\em if-and-only-if} its determinant is non-zero.

\item Null Space (Kernel): The {\em Null Space} of a matrix ${\cal N}(A)$ is the set of vectors $\bar{x}$ where $A\bar{x}=\bar{0}$

\item Positive Definite: For function $V(\bar{x})>0, \forall x | x\neq0$ (Stability Concepts (I), Page 9). A matrix is positive definite if all the eigen-values are greater than zero. 

\item Positive Semi-Definite: For function $V(\bar{x})\geq0, \forall x | x\neq0$ A matrix is positive semi-definite if all the eigen-values are greater than or equal to zero. 

\item Range: The {\em Range} of a matrix ${\cal R}(A)$ is the span of all the linearly independent column vectors in matrix $A$.

\item Rank: The maximum number of linearly independent column (or row) vectors in a matrix. ({\em Linear Systems Theory}, Page 257) It is the { \em Dimension} of the {\em Range} (SIE 550 Week 1 Lecture Notes page 6).

\item Semi-Definite Negative: For function $V(\bar{x})\leq0$ (Stability Concepts (II), Page 3). 

\item Similar: Matricies $A$ and $B$ are similar if $\exists P | B=P^{-1}AP$.

\item Span: The set of all linear combinations of vectors in a set. 

\item Transition Matrix (Fundamental Matrix): $\phi(t,\tau)=e^{A(t-\tau)}$

\end{itemize}

\newpage
\section*{Basic Linear Algebra}
\subsection*{Row Echelon Form}
A matrix is in Row Echelon Form if:
\begin{itemize}
	\item The first non-zero element in each row is a 1
	\item The Each leading entry is to the right of the previous row
	\item Rows with all zero elements are below rows having non-zero elements
\end{itemize}
\noindent
The following matrix is an example of row echelon form:
$$
\begin{bmatrix}
	1 & 2 & -1 & 0 & 5 \\
	0 & 0 & 1 & 3 & 1 \\
	0 & 0 & 0 & 1 & -3 
\end{bmatrix} 
$$

\subsection*{Row Reduced Echelon Form}
A matrix is in Row Reduced Echelon Form if:
\begin{itemize}
	\item The matrix meets the conditions of Row Echelon Form
	\item Every leading one is the only non-zero value in its column
\end{itemize}
\noindent
The following matrix is an example of row reduced echelon form:
$$
\begin{bmatrix}
	1 & 2 & 0 & 0 & 15 \\
	0 & 0 & 1 & 0 & 10 \\
	0 & 0 & 0 & 1 & -3 
\end{bmatrix} 
$$

\noindent
Matricies can be put in Row Echelon Form and Row Reduced Echelon form using Gauss Elimination.

\subsection*{Finding Rank}
To find the {\em Rank} of a matrix (the {\em dimension} of the {\em range}) put the matrix in row echelon form using Gauss Elmination and the number of columns with a leading 1 is the {\em Rank}

\subsection*{Finding Dimension of Null Space}
To find the number of linearly independent vectors in the null space, find the rank of the matrix, subtract the number of columns in the matrix by the rank, that is the dimension of the null space, the number of linearly independent vectors which allows the linear equation to be zero.

This is based on the theorem on page 7 of the SIE 550 Week 1 lecture notes where:
$$L:E_n \rightarrow E_m \text{ \& } dim(E_n)=n$$
then 
$$dim(E_n)=dim({\cal N}(L))+dim({\cal R}(L))$$

\subsection*{Eigenvalues}
Eigenvalues are all the $\lambda$s that satisfy the following equation:
$$det(A-\lambda I)=0$$

\subsection*{Eigenvectors}
Eigenvectors are vectors $\bar{v_n}$ which satisfy the following equation. 
$$(A-\lambda_n I)\bar{v_n}=\bar{0}$$

\noindent
The dimension of $\bar{v}$, for a given $\lambda$, will be one unless you are solving for the cases where there are repeated eigenvalues.

\subsection*{Finding Eigenvectors For Defective Matrices}
Often when finding eigenvectors for repeated eigenvalues to satisfy $A\bar{v}=\bar{0}$ one is presented with either two or more independent vector equations or a parameter which is independent of the others. If that fails, use the following equations:
Given that $\lambda$ is repeated $n$ times. $v_i$ can be found with:
$$(A-\lambda I)\bar{v_1}=\bar{0}$$
$$(A-\lambda I)\bar{v_2}=\bar{v_1}$$
$$(A-\lambda I)\bar{v_3}=\bar{v_2}$$
And so on...\\
\noindent
Eigenvectors of A can be found with the following Matlab command:\\
$[V,D] = eig(A)$\\
Where V is a matrix made of the right eigenvectors of the matrix. \\
\noindent
{\em Note}: No single approach above works all time. You must be cognizant of whether the eigenvectors makes sense for the system being solved.

\subsection*{Diagonal Matrix}
If a matrix is diagnonizable then the following is true
$$
D=M^{-1}AM
$$

\noindent
Where the diagonal matrix $D$ is composed of the eigenvalues $A$ such that:
$$
\begin{bmatrix}
	\lambda_1 & 0 & \dots & 0 \\
	0 & \lambda_2 & \dots & 0 \\
	\vdots & \vdots & \ddots & \vdots \\
	0 & 0 & \dots & \lambda_n \\
\end{bmatrix}
$$

\noindent 
M is composed of the eigenvectors of $A$:
$$
M = 
\begin{bmatrix}
	\bar{v_1} & \bar{v_2} & \dots & \bar{v_n}
\end{bmatrix}
$$
\noindent
Where eigenvector $\bar{v_n}$ corresponds to eigenvalue $\lambda_n$.

\subsection*{Finding Solution for Non-diagonizable Matrices}
If you want to find the solution to 
$$\dot{\bar{x}}=A\bar{x}$$
And A is diagonizable, the solution is:
$$\bar{x}(t)=e^{At}x_0=Me^{Dt}M^{-1}x_0$$

\subsection*{Finding Solution for Non-diagonizable Matrices}
If you have a Jordan Block, a solution will be in the form:
$$\bar{x}(t)=e^{At}x_0=Me^{Jt}M^{-1}x_0$$
$e^{Jt}$ can be decomposed as:
$$e^{J_it}=e^{(\lambda_iI+N_i)t}=e^{\lambda_it}e^{N_it}$$
Where $e^{\lambda_it}$ is a scalar value and $e^{N_it}$ is a matrix. $N_i$ is in the form:

$$
N_i=\begin{bmatrix} 
0 & 1 & 0 & \dots & 0 \\
0 & 0 & 1 & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0\\
\end{bmatrix}
$$

$e^{N_it}$ can be found with the following equation.

$$e^{N_it}=\begin{bmatrix} 1 & t & \frac{t^2}{2} & \dots & \frac{t^{n-1}}{(n-1)!} \\
0 & 1 & t & \dots & \frac{t^{n-2}}{(n-2)!} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & 1 & t \\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}$$ 

Example: If your J matrix looks like this:
$$\begin{bmatrix}
-1 & 0 & 0 \\
0 & -2 & 1 \\
0 & 0 & -2
\end{bmatrix}$$

Then:
$$e^{\lambda_it}e^{N_it} = 
\begin{bmatrix}
e^{-t} & 0 & 0 \\
0 & e^{-2t} & 0 \\
0 & 0 & e^{-2t}
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & t \\
0 & 0 & 1
\end{bmatrix}
$$
\section*{Linearization}
A non-linear system of equations cannot be analyzed with a system of linear equations, but it can be {\em linearized} around the equilibrium points into a set of linear equations. \\
Equilibrium points are all places where all derivatives of all functions are zero. \\
Steps linearizing a system:
\begin{enumerate}
	\item Find all equilibrium points. Find all $\bar{x}$ where $\dot{\bar{f}}(\bar{x}) = \bar{0}$
	\item Find the linearized matrix. 
	$A=\begin{bmatrix}
		\frac{\partial f_1(\bar{x})}{\partial x_1} & \frac{\partial f_1(\bar{x})}{\partial x_2} \\
		\frac{\partial f_2(\bar{x})}{\partial x_1} & \frac{\partial f_2(\bar{x})}{\partial x_2}
	\end{bmatrix}$
	\item Find the behavior of each equilibrium point by plugging it into the linearized matrix.
\end{enumerate}

The behavior of the equilibrium point depends on the eigenvalues of the matrix for the point. 

\begin{itemize}
	\item All eigenvalues real and negative: Stable Node (path goes directly towards equilibrium point)
	\item At-least one eigenvalue is positive and all are real: Saddle (unstable)
	\item All eigenvalues are real and positive: Unstable Node (path goes directly away from equilibrium point)
	\item Eigenvalues are complex and real part is negative: Stable Focus (path spirals towards equilibrium point).
	\item Eigenvalues are complex and real part is positive: Unstable Focus (path spirals away from equilibrium point).
	\item Eigenvalues are purely imaginary: Center Equilibrium Point (path goes around equilibrium point but does not go towards or away from it).
\end{itemize}

\newpage
\section*{Stability Theorems}
\subsection*{Stability of Autonomous Systems, $\dot{\bar{x}}=\bar{f}(\bar{x})$}
\begin{itemize}
\item Stable (LaSalle theorem) (Stability Concepts (I), page 11): a time invariant system is {\em stable} when it meets the following criteria. There exists a {\em Lyapunov function} where:
	\begin{itemize}
	\item $V(\bar{x})>0, \forall x \in D/\{\bar{0}\}$
	\item $V(\bar{0})=\bar{0}$
	\item $V(\bar{x})\leq0, \forall x \in D$
	\end{itemize}
\item Asymptotically Stable if:
	\begin{itemize}
	\item The criteria for stable are met and
	\item $V(\bar{x})<0, \forall x \in D/\{\bar{0}\}$
	\end{itemize}
\item Globally Asymptotically Stable if:
	\begin{itemize}
	\item The criteria for Asymptotically Stable are met and
	\item $V(\bar{x})\to\infty$ as $||\bar{x}||\to\infty$
	\end{itemize}
\end{itemize}

\newpage
\subsection*{Stability of Non-Autonomous Systems, $\dot{\bar{x}}=\bar{f}(\bar{x},t)$}
\begin{itemize}
\item Stable (Barbalat's Lemma) (Stability Concepts (II), page 10): a time variant system is {\em stable} when it meets the following criteria. There exists a {\em Lyapunov function} where:
	\begin{itemize}
	\item $V(\bar{x},t)>0$ is positive definite
	\item $\dot{V}(\bar{x},t)$ is negative semi-definite 
	\end{itemize}
\item Uniformly Stable where:
	\begin{itemize}
	\item The Criteria for Stable are met and
	\item $V(\bar{x},t)$ is {\em Decrescent}
	\end{itemize}
\item Uniformly Asymptotically Stable
	\begin{itemize}
	\item The Criteria for Uniform Stability are met and
	\item $\dot{V}(\bar{x},t)<0,\forall x \in D/\{\bar{0}\}$
	\end{itemize}
\item Globally Uniformly Asymptotically Stable:
	\begin{itemize}
	\item The Criteria for Uniform Asymptotically Stable are met and
	\item $V(\bar{x})\to\infty$ as $||\bar{x}||\to\infty$
	\end{itemize}
\item Exponential Stability (Stability Concepts (II), page 20):
	\begin{itemize}
	\item $V(\bar{x},t)\geq\alpha(||\bar{x}||)>0$
	\item $\dot{V}(\bar{x},t)\leq-\gamma(||\bar{x}||)$
	\item $V(\bar{x},t)\leq\beta(||\bar{x}||)$
	\item such that:
		\begin{itemize}
		\item $\alpha(||\bar{x}||)=k_1||\bar{x}||^a$
		\item $\beta(||\bar{x}||)=k_2||\bar{x}||^a$
		\item $\gamma(||\bar{x}||)=k_3||\bar{x}||^a$
		\end{itemize}
	\end{itemize}
\item Globally Exponentially Stable
	\begin{itemize}
	\item Exponential Stability criteria apply and:
	\item $c_1||\bar{x}||^2\leq V(\bar{x},t)\leq c_2||\bar{x}||^2$
	\end{itemize}
\item Finite Time Stability (Week 7 Lecture notes, page 7)
	\begin{itemize}
	\item There exist a continuously differentiable Lyapunov Function and a function $R(z)$ where $\dot{V}(\bar{x},t)\leq-R(V(\bar{x},t))$
	\item $\int_0^E\frac{dz}{R(z)}<+\infty$ for $E>0$
	\end{itemize}
\item Uniform Finite Time Stability
	\begin{itemize}
	\item The criteria for Finite Time Stability are met and: 
	\item $V(\bar{x},t)$ is {\em decrescent}.
	\end{itemize}
\item Globally Finite Time Stable 
	\begin{itemize}
	\item The criteria for Uniform Finite Time Stability are met and: 
	\item $V(\bar{x},t)$ is {\em Radially Unbounded}
	\end{itemize}
\item Finite Time Stability Converse (Week 7 Lecture notes, page 16)
	 \begin{itemize}
	 \item A system is {\em not} Finite time stable if:
	 \item There exist a continuously differentiable Lyapunov Function and a function $S(z)$ where $\dot{V}(\bar{x},t)\leq-S(V(\bar{x},t))$
	 \item $\int_0^E\frac{dz}{S(z)}=+\infty$ for $E>0$
	 \end{itemize}

\end{itemize}

\newpage
\section*{Solutions to Time Varying Linear Systems (Non-Autonomous)}
Consider systems in the form 
$$ 
\begin{cases}
\dot{\bar{x}}=A(t)\bar{x} \\
\bar{x}(t_0)=\bar{x}_0
\end{cases}
$$
where $\{a_{ij}(t)\}$ are continuous on $[t_0,t_0+T]$ \\

\noindent
The solution is in the form (systems theory week 3 page 17):
$$\bar{x}(t)=\Phi(t,t_0)\bar{x}_0$$

\noindent
where the {\em Transition Matrix} is in the form:

$$ \Phi(t,t_0) = I + \int_{t_0}^t A(\tau_1)d\tau_1+\int_{t_0}^tA(\tau_1)\int_{t_0}^{\tau_1}A(\tau_2)d\tau_2d\tau_1+\dots$$

\noindent
Note that for {\em Linear Time Invariant} Systems $\dot{\bar{x}}=A\bar{x}$ 
$$\bar{x}(t)=e^{At}\bar{x}(0)\rightarrow \Phi(t,0)=e^{At}$$

\subsection*{Properties of $ \Phi(t,\tau)$}

\begin{itemize}
	\item $\Phi(t,t) = I$
	\item $\frac{\partial}{\partial t}\Phi(t,\tau)=A(t)\Phi(t,\tau)$
	\item $\frac{\partial}{\partial t}\Phi(t,\tau)=-\Phi(t,\tau)A(t)$
	\item $\Phi(t,\tau)=\Phi(t,\sigma)\Phi(\sigma,\tau)$
	\item $\Phi(t,\tau)$ is non-singular for all t, $\tau$
	\item $\Phi^{-1}(t,\tau)=\Phi(\tau,t)$
\end{itemize}

\subsection*{Special Cases}
If $A(t)$ is a triangular matrix:

$$ A(t) = 
\begin{bmatrix}
a_{11}(t) & a_{12}(t) & a_{13}(t) \\
0 & a_{22}(t) & a_{23}(t) \\
0 & 0& a_{33}(t)\\
\end{bmatrix}
$$
Then so too is $\Phi(t,t_0)$:
$$\Phi(t,t_0) = 
\begin{bmatrix}
\phi_{11}(t,t_0) & \phi_{12}(t,t_0) & \phi_{13}(t,t_0)\\
0 & \phi_{22}(t,t_0) & \phi_{23}(t,t_0) \\
0 & 0 & \phi_{33}(t,t_0)\\
\end{bmatrix}
$$

\noindent
The diagonal transition functions can be found with this equation:
$$\phi_{ii}(t,t_0)=e^{\int_{t_0}^ta_{ii}(\tau)d\tau}$$

\section*{Controllability}
\subsection*{Theorem 5.1 ({\em Linear Systems Theory} Page. 249)}
The continuous linear system is controllable from any initial state $x(t_0)=x_0$ to an arbitrary state $x_1$ at time $t_1>t_0$ if and only if matrix $W(t_0,t_1)$ is nonsingular.

\subsection*{Controllability Gramian}
$$W(t_0,t_1)=\int_{t_0}^{t_1}\phi(t_0,\tau)B(\tau)B^T(\tau)\phi^T(t_0,\tau)d\tau$$

\subsection*{Theorem 5.2 ({\em Linear Systems Theory} Page. 251)}
Matrix $W(t_0,t_1)$ satisfies the following properties:
\renewcommand{\labelenumiii}{\Roman{enumii}}
\begin{enumerate}
  \item {\em It is symmetric.}
  \item {\em It is positive semi-definite}. 
  \item $(\partial/\partial t)W(t,t_1)=A(t)W(t,t_1)+W(t,t_1)A^T(t)-B(t)B^T(t)$, \\$W(t_1,t_1)=0$
  \item $W(t_0,t_1)=W(t_0,t)+\phi(t_0,t)W(t,t_1)\phi^T(t_0,t)$
\end{enumerate}

\subsection*{Theorem 5.3 ({\em Linear Systems Theory} Page. 254)}
Assume that with some positive integer $q$, $B(t)$ is q-times continuously differentiable, and $A(t)$ is $(q-1)$-times continuously differentiable on the interface $[t_0,t_1]$, furthermore for some $t*\in [t_0,t_1]$,
$$rank(K_0(t*),K_1(t*),\dots,K_q(t*))=n$$
Then the following system
$$\dot{\bar{x}}=A(t)\bar{x}+B(t)u,\quad \bar{x}(t_0)=x_0$$
is controllable. 

\subsection*{Theorem 5.4 ({\em Linear Systems Theory} Page. 258)}
The time-invariant continuous linear system is completely controllable if and only if the rank of the controllability matrix $K$ equals n.
$$K=(B,AB,A^2B,\dots,A^{n-1}B)$$
$rank(K)=n$ $\rightarrow$ Fully controllable (completly).
 

\newpage
\section*{Observability}

\subsection*{Equation 5.6 ({\em Linear Systems Theory} Page. 296)}
$$M(t_0,t_1)=\int_{t_0}^{t_1}\phi^T(\tau,t_0)C^T(\tau)C(\tau)\phi(\tau,t_0)d\tau$$

\subsection*{Theorem 6.1 ({\em Linear Systems Theory} Page. 296)}
It is possible to determine $x_0$ with in an additive constant vector, which is in $N(M(t_0,t_1))$. If $M(t_0,t_1)$ is nonsingular, then $x_0$ can be determined uniquely.

\subsection*{Theorem 6.2 ({\em Linear Systems Theory} Page. 299)}
Matrix $M(t_0,t_1)$ satisfies the following properties:
\renewcommand{\labelenumiii}{\Roman{enumii}}
\begin{enumerate}
  \item {\em It is symmetric.}
  \item {\em It is positive semi-definite}. 
  \item $(\partial/\partial t)M(t,t_1)=-A(t)M(t,t_1)-M(t,t_1)A(t)-C^T(t)C(t)$, \\$M(t_1,t_1):0$
  \item $M(t_0,t_1)=M(t_0,t)+\phi(t,t_0)M(t,t_1)\phi^T(t,t_0)$
\end{enumerate}

\subsection*{Theorem 6.3 ({\em Linear Systems Theory} Page. 300)}
The time-invariant continuous linear system is observable for arbitrary $t_1>t_0$ if and only if the rank of the observability matrix $L$ equals n.

$$L=\begin{bmatrix}
C \\
CA \\
CA^2 \\
CA^{m-1}
\end{bmatrix}
$$
$rank(L)=n$ $\rightarrow$ Fully observable.
\subsection*{Theorem 6.4 ({\em Linear Systems Theory} Page. 302)}
Assume that the rank r of matrix $L$ is less than n. Then there exists a nonsingular matrix $T$ such that

$$\bar{A}=TAT^{-1}=\begin{pmatrix}
\bar{A}_{11} & O \\
\bar{A}_{21} & \bar{A}_{22}
\end{pmatrix}
$$

$$\bar{B}=TB=\begin{pmatrix}
\bar{B}_{1}\\
\bar{B}_{2}
\end{pmatrix}
$$

$$\bar{A}=CT^{-1}=\begin{pmatrix}
\bar{C}_{1} & O 
\end{pmatrix}
$$

\noindent
where the sizes of matrices $\bar{A}_{11}$, $\bar{A}_{21}$, $\bar{A}_{22}$ are $r\times r, (n-r)\times r, (n-r)\times (n-r),$ respectively, and $\bar{B}_1$ has r rows and $\bar{C}_1$ has r columns. Furthermore,
\renewcommand{\labelenumiii}{\Roman{enumii}}
\begin{enumerate}
  \item system $(\bar{A}_{11}, \bar{B}_1, \bar{C}_1)$ is completely observable, and
  \item the transfer function of systems $(A, B, C)$ and $(\bar{A}_{11}, \bar{B}_1, \bar{C}_1)$ coincide.
\end{enumerate}

\subsection*{Theorem 6.5 ({\em Linear Systems Theory} Page. 303)}
System $(A, B, C)$ is completely observable if and only if matrix $A$ has not eigenvector $q$ that is orthogonal to the rows of $C$.

\subsection*{Theorem 6.6 ({\em Linear Systems Theory} Page. 304)}
It is possible to determine $x_0$ within an additive constant vector, which is in $N(M(0,t_1))$. If $M(0,t_1)$ is nonsingular, then $x_0$ can be determined uniquely. 

\subsection*{Theorem 6.7 ({\em Linear Systems Theory} Page. 304)}
The time-invariant discrete linear system is observable at arbitrary $t_1\geq n$ if and only if the rank of the observability matrix $L$ equals n.

\subsection*{Primal vs. Dual ({\em SIE550\_FinalReview} Page 2)}
The {\em dual} of the time-invariant continuous system
$$P_c:\dot{\bar{x}}=A\bar{x}+B\bar{u}$$
$$y=C\bar{x}$$
is given as
$$D_c:\dot{\bar{z}}=A^T\bar{z}+C^T\bar{v}$$
$$w=B^T\bar{z}$$

P is completely controllable $\leftrightarrow$ D is completely observable.
$$K_p^T=L_D$$

\section*{Canonical Forms}

\subsection*{Computed Form}
Assume that system $(A, b, c^T)$ is completely controllable, then it can be transformed into an $(\tilde{A}, \tilde{b}, \tilde{c}^T)$-system, where \\
$\tilde{A}=
\begin{pmatrix}
	0 & 0 & 0 & \dots & 0 & a_0 \\
	1 & 0 & 0 & \dots & 0 & a_1 \\
	0 & 1 & 0 & \dots & 0 & a_2 \\
	\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & 0 & \dots & 1 & a_{n-1} 
\end{pmatrix}$ and 
$\tilde{b}=\begin{pmatrix}
1\\
0\\
0\\
\vdots\\
0
\end{pmatrix}$

$$K=(b,Ab,A^2b,\dots,A^{n-1}b)$$
$$T = K^{-1}$$
$$\tilde{A}=TAT^{-1}$$
$$\tilde{b}=Tb$$
$$\tilde{c}^T=C^TT^{-1}$$

\subsection*{Assembled Form}
$\tilde{A}=
\begin{pmatrix}
	0 & 1 & 0 & \dots & 0 \\
	0 & 0 & 1 & \dots & 0 \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	0 & 0 & 0 & \dots & 1 \\
	a_0 & a_1 & a_2 & \dots & a_{n-1} 
\end{pmatrix}$ and 
$\tilde{b}=\begin{pmatrix}
0\\
0\\
\vdots\\
0\\
1
\end{pmatrix}$

$$K=(b,Ab,A^2b,\dots,A^{n-1}b)$$

$$\begin{pmatrix}
	t_1^T\\
	t_2^T\\
	\vdots\\
	t_n^T
\end{pmatrix} = K^{-1}$$

$$T=\begin{pmatrix}
	t_n^T\\
	t_n^TA\\
	t_n^TA^2\\
	\vdots\\
	t_n^TA^{n-1}\\
\end{pmatrix}$$

$$\tilde{A}=TAT^{-1}$$
$$\tilde{b}=Tb$$
$$\tilde{c}^T=C^TT^{-1}$$

\section*{Estimation and Design}
\subsection*{Theorem 9.1}
To find the matrix $K$ follow the algorithm below
\begin{itemize}
	\item Find the cannonical form of the system. 
\end{itemize}

\end{document}
